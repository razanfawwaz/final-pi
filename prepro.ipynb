{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18177bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d50ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah URL:  4000\n"
     ]
    }
   ],
   "source": [
    "# Baca dua file teks yang berisi daftar URL\n",
    "with open('links_hiburan.txt', 'r') as file1, open('links_lifestyle.txt', 'r') as file2:\n",
    "    urls1 = [line.strip() for line in file1]\n",
    "    urls2 = [line.strip() for line in file2]\n",
    "\n",
    "# Gabungkan kedua list URL menjadi satu\n",
    "urls = urls1 + urls2\n",
    "print('Jumlah URL: ', len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc24fe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 url done\n",
      "100 50 url done\n",
      "150 50 url done\n",
      "200 50 url done\n",
      "250 50 url done\n",
      "300 50 url done\n",
      "350 50 url done\n",
      "400 50 url done\n",
      "450 50 url done\n",
      "500 50 url done\n",
      "550 50 url done\n",
      "600 50 url done\n",
      "650 50 url done\n",
      "700 50 url done\n",
      "750 50 url done\n",
      "800 50 url done\n",
      "850 50 url done\n",
      "900 50 url done\n",
      "950 50 url done\n",
      "1000 50 url done\n",
      "1050 50 url done\n",
      "1100 50 url done\n",
      "1150 50 url done\n",
      "1200 50 url done\n",
      "1250 50 url done\n",
      "1300 50 url done\n",
      "1350 50 url done\n",
      "1400 50 url done\n",
      "1450 50 url done\n",
      "1500 50 url done\n",
      "1550 50 url done\n",
      "1600 50 url done\n",
      "1650 50 url done\n"
     ]
    }
   ],
   "source": [
    "skipped_urls = []\n",
    "urldoc = {}\n",
    "\n",
    "for url in urls:\n",
    "    # Kirim permintaan GET\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse konten dengan BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Temukan elemen dengan atribut itemprop=\"articleBody\"\n",
    "    article_body = soup.find('div', class_=\"post-content\")\n",
    "\n",
    "    # Jika elemen tidak ditemukan, lewati url dan tambahkan ke list\n",
    "    if article_body is None:\n",
    "        skipped_urls.append(url)\n",
    "        continue\n",
    "\n",
    "    # Dapatkan teks dari setiap elemen <p> dan simpan ke dalam list\n",
    "    paragraphs = (' '.join([p.text for p in soup.find('div', class_=\"post-content\").descendants])).strip().replace(\"\\n\", \"\").replace(\"\\xa0\", \"\").replace(\"\\r\", \"\").split(\".\")\n",
    "\n",
    "    # Gabungkan list menjadi satu string, dengan setiap paragraf dipisahkan oleh baris baru\n",
    "    text = '\\n'.join(paragraphs)\n",
    "\n",
    "    # Hapus tanda baca dan karakter non-alfabet\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # Case folding\n",
    "    text = text.lower()\n",
    "\n",
    "    # Ekstrak judul halaman sebagai nama file (Anda mungkin perlu menyesuaikan ini tergantung pada struktur halaman web)\n",
    "    title = soup.title.string\n",
    "\n",
    "    # Buat versi singkat dari judul untuk digunakan sebagai nama file\n",
    "    short_title = title[:100]  # Ambil 100 karakter pertama dari judul\n",
    "\n",
    "    # Ganti karakter yang tidak valid dalam nama file\n",
    "    filename = \"\".join(c for c in short_title if c.isalnum() or c.isspace()).rstrip() + \".txt\"\n",
    "\n",
    "    urldoc[filename] = url\n",
    "\n",
    "    # Tentukan direktori tempat Anda ingin menyimpan file\n",
    "    directory = \"./data/\"\n",
    "\n",
    "    # Gabungkan direktori dan nama file untuk mendapatkan path lengkap ke file\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    # Simpan teks ke file .txt\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    # save link to json\n",
    "    with open('urldoc.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(urldoc, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "## every 50 url, print \"50 url done\"\n",
    "    if len(urldoc) % 50 == 0:\n",
    "        print(len(urldoc), \"50 url done\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
